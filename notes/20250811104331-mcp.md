# Model Context Protocol (MCP)

MCP is a protocol that standarises how applications provide context to large language models (LLMs).

A lot of people liken it to 'USB-C for AI' - a standardised way to connect AI models to different data sources and tools.

MCP is a client-server architecture, with a MCP Host (e.g. VS Code) connects to one or more MCP Servers (e.g. Figma's). The MCP Host does this by creating one-to-many MCP Clients, which then connect to a MCP Server in a one-to-one relationship.
- **MCP Host**: The AI application that coordinates and manages one or multiple MCP clients
- **MCP Client**: A component that maintains a connection to an MCP server and obtains context from an MCP server for the MCP host to use
- **MCP Server**: A program that provides context to MCP clients. MCP servers provide capabilities - access to a database, access to email, etc. 
The key word here is **context** - a MCP helps a LLM understand the context of what it's working with.

MCP works on two layers:
- **Data layer**: Defines the JSON-RPC based protocol for client-server communication, including lifecycle management, and core primitives, such as tools, resources, prompts and notifications.
- **Transport layer**: Defines the communication mechanisms and channels that enable data exchange between clients and servers, including transport-specific connection establishment, message framing, and authorization.
Conceptually the data layer is the inner layer, while the transport layer is the outer layer.

The data layer protocol is the most interesting. Here, MCP defines three **primitives**:
- **Tools**: Executable functions that AI applications can invoke to perform actions (e.g., file operations, API calls, database queries)
- **Resources**: Data sources that provide contextual information to AI applications (e.g., file contents, database records, API responses)
- **Prompts**: Reusable templates that help structure interactions with language models (e.g., system prompts, few-shot examples)

Each primitive type has associated methods for discovery (`*/list`), retrieval (`*/get`), and in some cases, execution (`tools/call`).

This is the key part, as AI agents can discover these primitives at **runtime**. Meaning in the same request, an MCP Client can discover and then use a given tool. This is sometimes known as 'dynamic self discovery'?

MCP is similar, to different than something like REST.
- Both are abstractions around ways to interface with a service
- Both are client-server architectures
- Crucially, MCP was purpose build for AI/LLMs - it provides context, data, and tools in ways that align with how AI agents operate
	- As such, it can bake in certain assumptions that are useful for AI, such as dynamic self discovery.
	- Standaridsation then becomes more important also, as REST is more of a 'style' than hard rules.
Note that MCP servers often are a wrapper around an existing API - translating between MCP and REST, for example.

It's kind of like **an** API, but not quite. If an API is just an interface or contract to execute functions, that's very similar to what **MCP servers** are trying to do (as a whole). However because MCP servers use MCP, there's certain rules and schema they have to follow. This makes MCP servers perhaps an opinionated API. Often MCP servers are just wrappers around an existing API, translating between MCP and REST, for example.

But remember, that's MCP servers. Being more nitpicky, MCP is a protocol, and APIs are an interface. So MCP != an API - but an MCP server is close to an API.

In theory MCP is universal to more than just AI, but then it gets a bit weird - what's actually universal? If you remove the AI, then you've just created the idea of an 'API', which we already knew was universal. However MCPs being the hot new thing is making everyone improve interoperability in a new way.

https://modelcontextprotocol.io/
https://glama.ai/blog/2025-06-06-mcp-vs-api

[[ai]]
[[api]]